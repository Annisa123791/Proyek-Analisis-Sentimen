# -*- coding: utf-8 -*-
"""google_play_scraper.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PNOzMaLDooODO4CR4e6paK7o0Q4-Yv5j
"""

pip install google-play-scraper

!pip install pipreqs

from google_play_scraper import app, Sort, reviews_all
import pandas as pd
import time

def scrape_duolingo_reviews():
    # Scrape reviews in batches
    all_reviews = []

    # Scrape multiple batches to get 3000+ reviews
    for _ in range(10):
        result = reviews_all(
            'com.duolingo',
            sleep_milliseconds=1000,
            lang='id',
            country='id',
            sort=Sort.MOST_RELEVANT,
            count=300
        )
        all_reviews.extend(result)
        time.sleep(2)  # Add delay between requests

    # Convert to DataFrame
    df = pd.DataFrame(all_reviews)

    # Remove duplicates
    df = df.drop_duplicates(subset=['reviewId'])

    # Save to CSV
    df.to_csv('duolingo_reviews.csv', index=False)
    print(f"Saved {len(df)} reviews to duolingo_reviews.csv")

if __name__ == "__main__":
    scrape_duolingo_reviews()

pip install sastrawi

pip install google-play-scraper

pip install textblob

pip install translate

pip install wordcloud

from google_play_scraper import Sort, reviews
from google_play_scraper import app
import pandas as pd
import re
import string
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from wordcloud import WordCloud
import nltk

# Pastikan Anda sudah mendownload resource nltk yang dibutuhkan
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# 1. Scraping data review Duolingo dari Play Store
result, _ = reviews(
    'com.duolingo',
    lang='id',  # Mengambil review dalam bahasa Indonesia dan Inggris
    country='id',
    sort=Sort.MOST_RELEVANT,
    count=2000
)

# Memuat data
data = pd.read_csv('/content/duolingo_reviews.csv', encoding='MacRoman')

# Menampilkan beberapa baris pertama
print(data.head(10))

# Menyimpan ulasan ke dalam DataFrame
data = pd.DataFrame(result)

# Memilih kolom yang diperlukan
data_reviews = data[['content', 'score']]

df = pd.DataFrame(np.array(result),columns=['reviews'])
df = df.join(pd.DataFrame(df.pop('reviews').tolist()))
df.head()

# mengubah nama atribute
df = df.rename(columns={'content' : 'ulasan', 'score':'rating'})
# untuk membaca
df.head()

# Cleaning Text
def clean_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Hapus URL
    text = re.sub(r'\@\w+|\#', '', text)  # Hapus mention dan hashtag
    text = re.sub(r'\d+', '', text)  # Hapus angka
    text = text.lower()  # Ubah ke huruf kecil
    return text

# Ganti 'review' dengan nama kolom yang benar jika berbeda
data['cleaned_text'] = data['content'].apply(clean_text)
print(data['cleaned_text'].head().to_markdown())

# Tokenisasi
data['tokens'] = data['cleaned_text'].apply(word_tokenize)
print(data['tokens'].head().to_markdown())

# Stopword Removal dan Custom Stopword
stop_words = set(stopwords.words('indonesian'))
custom_stopwords = {'masuk', 'login'}  # Tambahkan kata kustom
all_stopwords = stop_words.union(custom_stopwords)

data['filtered_tokens'] = data['tokens'].apply(lambda x: [word for word in x if word not in all_stopwords])
print(data['filtered_tokens'].head().to_markdown())

# Fungsi untuk melakukan preprocessing pada teks
def preprocess(text):
    # 1. Cleaning: Menghapus URL, mention, dan karakter non-alphabetic
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\@\w+|\#', '', text)  # Remove @mentions and hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters

# Stemming
factory = StemmerFactory()
stemmer = factory.create_stemmer()

data['stemmed'] = data['filtered_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])
print(data['stemmed'].head().to_markdown())

# Apply the slang detection function and store results in a new column
data['slangwords'] = data['tokens'].apply(detect_slang)

# Display the slangwords output as per your desired format
print(data['slangwords'].head().to_markdown())

# Kamus Positif dan Negatif
positive_dict = {
    'baik', 'bagus', 'menyenangkan', 'mudah', 'praktis', 'efektif',
    'menarik', 'berguna', 'keren', 'suka', 'puas', 'cepat',
    'kualitas', 'hebat', 'terbaik', 'inovatif', 'ramah', 'fasilitas'
}

negative_dict = {
    'buruk', 'jelek', 'sulit', 'bingung', 'tidak', 'masalah',
    'kesulitan', 'lambat', 'kekurangan', 'kekecewaan', 'error',
    'login', 'tidak bekerja', 'terganggu', 'bosen', 'kurang',
    'menyebalkan', 'frustrasi', 'berantakan', 'hampir'
}

# Sample data simulating 'cleaned_text' column (You can replace this with your real DataFrame)
data = pd.DataFrame({
    'cleaned_text': [
        "diupdate ngelag diupdate parah udah nyelesaiin ajar ngebug nggak hitung ngulang ajar",
        "ajar susun basic sampe rumit bahasa ajar bahasa hari ajar serius jadi game cocok",
        "suka aplikasi duolingo bantu ajar bahasa udah gratis visual nya bagus saran gua tolong tes",
        "ajar bahasa inggris sulit gak suka ajar seru bas game gak cuman ajar matematika",
        "seruu banget gk kursus les gk bahasa inggris lumayan lancar pakai bahasa inggris salah kasih"
    ]
})

# Tokenize each row in the 'cleaned_text' column
data['tokens'] = data['cleaned_text'].apply(word_tokenize)

# Detect Slang Words (Words not found in either dictionary)
def detect_slang(tokens):
    all_dict_words = positive_dict.union(negative_dict)  # Combine both dictionaries
    slang_words = [word for word in tokens if word not in all_dict_words]
    return slang_words

# Apply the slang detection function and store results in a new column
data['slangwords'] = data['tokens'].apply(detect_slang)

# Display the slangwords output as per your desired format
print(data['slangwords'].head().to_markdown())

# Fungsi untuk Menentukan Sentimen
def sentiment_analysis(row):
    rating = row['score']

    # Menentukan sentimen awal berdasarkan rating
    if rating > 3:
        sentiment = 'positif'
    elif rating < 3:
        sentiment = 'negatif'
    else:
        # Jika rating netral, cek apakah ada kata-kata yang mendukung positif atau negatif
        for word in row['stemmed']:
            if word in positive_dict:
                return 'positif'
            elif word in negative_dict:
                return 'negatif'
        # Jika tidak ada kata positif atau negatif, anggap sentimen netral sebagai 'positif'
        sentiment = 'positif'
    return sentiment

# Filter untuk hanya menampilkan sentimen positif dan negatif
filtered_counts = sentiment_counts[sentiment_counts.index.isin(['positif', 'negatif'])]

# Mencetak hasil
print(filtered_counts.to_markdown())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Pisahkan fitur dan label
X = data['cleaned_text']
y = data['sentiment']
# Ekstraksi fitur dengan TF-IDF
vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)

# Membagi data menjadi data latih dan uji
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, stratify=y, random_state=42)

pip install -U imbalanced-learn

pip install imbalanced-learn

from imblearn.over_sampling import SMOTE

# Menggunakan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
print("Distribusi kelas setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

# Melatih model dengan data yang sudah di-resample
model = MultinomialNB(alpha=1.0)
model.fit(X_train_smote, y_train_smote)

# Memprediksi hasil
y_pred = model.predict(X_test)

print(data['sentiment'].value_counts())

print(y_pred)

# Latih Model dengan Naive Bayes
X = data['cleaned_text']
y = data['sentiment']

vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# Menggunakan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
print("Distribusi kelas setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Splitting Data
X = data['cleaned_text']
y = data['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pisahkan data menjadi fitur dan label (sentimen)
X = data['cleaned_text']
y = data['sentiment']

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Menampilkan hasil ekstraksi fitur
features_df

# Ekstraksi Fitur menggunakan CountVectorizer
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Inisialisasi SMOTE dan terapkan pada data latih
smote = SMOTE(random_state=42)
X_train_smote_vec, y_train_smote_vec = smote.fit_resample(X_train_vec, y_train)

# Model Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_smote_vec, y_train_smote_vec)
y_pred = nb_model.predict(X_test_vec)

# Vectorizing the text data using TF-IDF
# Langkah awal - Split data dan ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8)
X_tfidf = tfidf.fit_transform(data['cleaned_text'])  # Ekstraksi fitur menggunakan TF-IDF
y = data['sentiment']  # Target atau label

# Membagi data menjadi data latih dan data uji (70:30)
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Inisialisasi SMOTE dan terapkan pada data latih
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Inisialisasi model Naive Bayes dan latih model dengan data yang telah di-resample
model = MultinomialNB()
model.fit(X_train_smote, y_train_smote)

# membagi data 70:30
X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['sentiment'], test_size=0.7, random_state=125)
vectorizer = TfidfVectorizer(max_features=200)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
# Terapkan SMOTE pada data latih untuk menangani ketidakseimbangan kelas
smote = SMOTE(random_state=42)
X_train_smote_tfidf, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)

# Inisialisasi dan pelatihan model Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_smote_tfidf, y_train_smote)

# Prediksi pada data train
y_train_pred = nb_model.predict(X_train_smote_tfidf)
train_accuracy = accuracy_score(y_train_smote, y_train_pred)

# Prediksi pada data test
y_test_pred = nb_model.predict(X_test_tfidf)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Menambahkan hasil ulasan klasifikasi ke dataframe
df['hasil_ulasan_klasifikasi_nb'] = nb_model.predict(vectorizer.transform(data['cleaned_text']))

# Matriks kebingungan dan laporan klasifikasi untuk data test
conf_matrix = confusion_matrix(y_test, y_test_pred)
class_report = classification_report(y_test, y_test_pred)

# Output hasil
print(f'Accuracy Train: {train_accuracy:.2f}\n')
print(f'Accuracy Test: {test_accuracy:.2f}\n')
print('Confusion Matrix:')
print(conf_matrix)
print('\nClassification Report:')
print(class_report)

from sklearn.model_selection import GridSearchCV

# Definisikan parameter grid untuk pencarian alpha terbaik
param_grid = {'alpha': [0.1, 0.5, 1, 1.5, 2]}
grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='f1_weighted')
grid_search.fit(X_train_smote_tfidf, y_train_smote)

# Gunakan model dengan parameter terbaik
best_nb_model = grid_search.best_estimator_
best_nb_model.fit(X_train_smote_tfidf, y_train_smote)

# Prediksi pada data test
y_test_pred = best_nb_model.predict(X_test_tfidf)
test_accuracy = accuracy_score(y_test, y_test_pred)
conf_matrix = confusion_matrix(y_test, y_test_pred)
class_report = classification_report(y_test, y_test_pred)

print(f'Accuracy Test after tuning: {test_accuracy:.2f}\n')
print('Confusion Matrix:')
print(conf_matrix)
print('\nClassification Report:')
print(class_report)

# Evaluasi dengan Cross Validation
scores = cross_val_score(model, X_vectorized, y, cv=5)
print('Cross-validation scores:', scores)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pip install seaborn

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()  # Menampilkan plot

print(data['sentiment'].value_counts())

print(y_pred)

import matplotlib.pyplot as plt

# Assuming X_train and X_test represent 70% and 30% of the data respectively
train_size = 70  # representing 70% of data
test_size = 30   # representing 30% of data

# Preparing data for visualization
sizes = [train_size, test_size]
labels = ['Training Data (70%)', 'Testing Data (30%)']
colors = ['#66b3ff', '#99ff99']

import matplotlib.pyplot as plt

# Assuming X_train and X_test represent 70% and 30% of the data respectively
train_size = 70  # representing 70% of data
test_size = 30   # representing 30% of data

# Preparing data for visualization
sizes = [train_size, test_size]
labels = ['Training Data (70%)', 'Testing Data (30%)']
colors = ['#66b3ff', '#99ff99']

# Creating pie chart
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, counterclock=False)
plt.title('Data Split Ratio: Training vs Testing')
plt.show()

import matplotlib.pyplot as plt

#ini adalah contoh jumlah sentimen yang telah dihitung
sentiment_counts = {
    'positif': 1800,
    'negatif': 150,
    'netral': 50  # Sentimen netral tidak akan ditampilkan
}

# 1. Pilih hanya sentimen positif dan negatif
filtered_sentiment = {key: sentiment_counts[key] for key in ['positif', 'negatif']}

# 2. Visualisasi - Diagram Batang Sentimen Positif dan Negatif
plt.figure(figsize=(8, 6))
plt.bar(filtered_sentiment.keys(), filtered_sentiment.values(), color=['gold', 'pink'])

# 3. Tambahkan judul dan label
plt.title('Distribusi Sentimen Positif dan Negatif')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')

# 4. Tampilkan plot
plt.show()

# Hitung jumlah setiap kategori sentimen
sentiment_counts = data['sentiment'].value_counts()

# Filter untuk hanya menampilkan sentimen positif dan negatif
filtered_sentiment_counts = sentiment_counts[sentiment_counts.index.isin(['positif', 'negatif'])]

# Visualisasi - Diagram Lingkaran (Pie Chart)
plt.figure(figsize=(8, 6))
plt.pie(filtered_sentiment_counts, labels=filtered_sentiment_counts.index, autopct='%1.1f%%',
        startangle=90, colors=['lightgreen', 'lightcoral'])
plt.title('Distribusi Sentimen (Positif dan Negatif)')
plt.axis('equal')  # Untuk membuat pie chart menjadi lingkaran sempurna
plt.show()

# Visualisasi Word Cloud
# Gabungkan token menjadi string untuk masing-masing sentimen
positive_text = ' '.join([' '.join(tokens) for tokens in data[data['sentiment'] == 'positif']['stemmed']])
negative_text = ' '.join([' '.join(tokens) for tokens in data[data['sentiment'] == 'negatif']['stemmed']])

# Membuat WordCloud untuk sentimen positif
wordcloud_positive = WordCloud(width=800, height=400, background_color='black', colormap='Greens').generate(positive_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud - Sentimen Positif')
plt.show()

# Membuat WordCloud untuk sentimen negatif
wordcloud_negative = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(negative_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud - Sentimen Negatif')
plt.show()