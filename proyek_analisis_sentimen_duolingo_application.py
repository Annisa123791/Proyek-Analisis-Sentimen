# -*- coding: utf-8 -*-
"""Proyek Analisis Sentimen Duolingo Application.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XJsHxuPhVMQkRBtyqMnBv6EKP22ZWrdM
"""

!pip install pipreqs

# Commented out IPython magic to ensure Python compatibility.
# Install the googletrans module
# %pip install googletrans==4.0.0-rc1

pip install sastrawi

pip install google-play-scraper

pip install textblob

pip install translate

pip install wordcloud

from google_play_scraper import Sort, reviews
from google_play_scraper import app
import pandas as pd
import re
import string
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from wordcloud import WordCloud
import nltk

# Pastikan Anda sudah mendownload resource nltk yang dibutuhkan
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# 1. Scraping data review Duolingo dari Play Store
result, _ = reviews(
    'com.duolingo',
    lang='id',  # Mengambil review dalam bahasa Indonesia dan Inggris
    country='id',
    sort=Sort.MOST_RELEVANT,
    count=4000
)

# Memuat data
data = pd.read_csv('/content/duolingo_reviews.csv', encoding='MacRoman')

# Menampilkan beberapa baris pertama
print(data.head(10))

"""# Bagian Baru

# Bagian Baru
"""

# Menyimpan ulasan ke dalam DataFrame
data = pd.DataFrame(result)

# Memilih kolom yang diperlukan
data_reviews = data[['content', 'score']]

df = pd.DataFrame(np.array(result),columns=['reviews'])
df = df.join(pd.DataFrame(df.pop('reviews').tolist()))
df.head()

# mengubah nama atribute
df = df.rename(columns={'content' : 'ulasan', 'score':'rating'})
# untuk membaca
df.head()

# Cleaning Text
def clean_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # Hapus URL
    text = re.sub(r'\@\w+|\#', '', text)  # Hapus mention dan hashtag
    text = re.sub(r'\d+', '', text)  # Hapus angka
    text = text.lower()  # Ubah ke huruf kecil
    return text

# Ganti 'review' dengan nama kolom yang benar jika berbeda
data['cleaned_text'] = data['content'].apply(clean_text)
print(data['cleaned_text'].head().to_markdown())

# Tokenisasi
data['tokens'] = data['cleaned_text'].apply(word_tokenize)
print(data['tokens'].head().to_markdown())

# Stopword Removal dan Custom Stopword
stop_words = set(stopwords.words('indonesian'))
custom_stopwords = {'masuk', 'login'}  # Tambahkan kata kustom
all_stopwords = stop_words.union(custom_stopwords)

data['filtered_tokens'] = data['tokens'].apply(lambda x: [word for word in x if word not in all_stopwords])
print(data['filtered_tokens'].head().to_markdown())

# Stemming
factory = StemmerFactory()
stemmer = factory.create_stemmer()

data['stemmed'] = data['filtered_tokens'].apply(lambda x: [stemmer.stem(word) for word in x])
print(data['stemmed'].head().to_markdown())

# Kamus Positif dan Negatif
positive_dict = {
    'baik', 'bagus', 'menyenangkan', 'mudah', 'praktis', 'efektif',
    'menarik', 'berguna', 'keren', 'suka', 'puas', 'cepat',
    'kualitas', 'hebat', 'terbaik', 'inovatif', 'ramah', 'fasilitas'
}

negative_dict = {
    'buruk', 'jelek', 'sulit', 'bingung', 'tidak', 'masalah',
    'kesulitan', 'lambat', 'kekurangan', 'kekecewaan', 'error',
    'login', 'tidak bekerja', 'terganggu', 'bosen', 'kurang',
    'menyebalkan', 'frustrasi', 'berantakan', 'hampir'
}

# Fungsi untuk Menentukan Sentimen
def sentiment_analysis(row):
    rating = row['score']

    # Menentukan sentimen awal berdasarkan rating
    if rating > 3:
        sentiment = 'positif'
    elif rating < 3:
        sentiment = 'negatif'
    else:
        # Jika rating netral, cek apakah ada kata-kata yang mendukung positif atau negatif
        for word in row['stemmed']:
            if word in positive_dict:
                return 'positif'
            elif word in negative_dict:
                return 'negatif'
        # Jika tidak ada kata positif atau negatif, anggap sentimen netral sebagai 'positif'
        sentiment = 'positif'

    return sentiment

# Menerapkan fungsi sentiment_analysis ke DataFrame
data['sentiment'] = data.apply(sentiment_analysis, axis=1)

# Menghitung jumlah setiap kategori sentimen
sentiment_counts = data['sentiment'].value_counts()

# Filter untuk hanya menampilkan sentimen positif dan negatif
filtered_counts = sentiment_counts[sentiment_counts.index.isin(['positif', 'negatif'])]

# Mencetak hasil
print(filtered_counts.to_markdown())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Pisahkan fitur dan label
X = data['cleaned_text']
y = data['sentiment']

# Ekstraksi fitur dengan TF-IDF
vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)

# Membagi data menjadi data latih dan uji
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, stratify=y, random_state=42)

pip install -U imbalanced-learn

pip install imbalanced-learn

from imblearn.over_sampling import SMOTE

# Menggunakan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
print("Distribusi kelas setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

# # Melatih model dengan Multinomial Naive Bayes
# model = MultinomialNB(alpha=1.0)
# model.fit(X_train, y_train)

# Melatih model dengan data yang sudah di-resample
model = MultinomialNB(alpha=1.0)
model.fit(X_train_smote, y_train_smote)

# Memprediksi hasil
y_pred = model.predict(X_test)

print(data['sentiment'].value_counts())

print(y_pred)

# Visualisasi Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=['negatif', 'positif'])
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=['Negatif', 'Positif'],
            yticklabels=['Negatif', 'Positif'], annot_kws={"size": 12, "color": "black"})
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix')
plt.show()

# Menampilkan laporan klasifikasi
print(metrics.classification_report(y_test, y_pred))

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8)
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi DataFrame
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Membagi data menjadi data latih (70%) dan data uji (30%)
X_train, X_test, y_train, y_test = train_test_split(features_df, y, test_size=0.3, random_state=42)

# Menggunakan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Menampilkan hasil ekstraksi fitur
features_df

# Latih Model dengan Naive Bayes
X = data['cleaned_text']
y = data['sentiment']

vectorizer = TfidfVectorizer()
X_vectorized = vectorizer.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# Menggunakan SMOTE untuk oversampling kelas minoritas
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
print("Distribusi kelas setelah SMOTE:")
print(pd.Series(y_train_smote).value_counts())

model = MultinomialNB()
model.fit(X_train_smote, y_train_smote)

y_pred = model.predict(X_test)
print(metrics.classification_report(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Membagi data menjadi data latih (80%) dan data uji (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Splitting Data
X = data['cleaned_text']
y = data['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pisahkan data menjadi fitur dan label (sentimen)
X = data['cleaned_text']
y = data['sentiment']

# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)

# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())

# Menampilkan hasil ekstraksi fitur
features_df

# Ekstraksi Fitur menggunakan CountVectorizer
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Inisialisasi SMOTE dan terapkan pada data latih
smote = SMOTE(random_state=42)
X_train_smote_vec, y_train_smote_vec = smote.fit_resample(X_train_vec, y_train)

# Model Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_smote_vec, y_train_smote_vec)
y_pred = nb_model.predict(X_test_vec)

# Vectorizing the text data using TF-IDF
# Langkah awal - Split data dan ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8)
X_tfidf = tfidf.fit_transform(data['cleaned_text'])  # Ekstraksi fitur menggunakan TF-IDF
y = data['sentiment']  # Target atau label

# Membagi data menjadi data latih dan data uji (70:30)
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Inisialisasi SMOTE dan terapkan pada data latih
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Inisialisasi model Naive Bayes dan latih model dengan data yang telah di-resample
model = MultinomialNB()
model.fit(X_train_smote, y_train_smote)

# Prediksi pada data uji
y_test_pred = model.predict(X_test)

# Evaluasi model
accuracy = accuracy_score(y_test, y_test_pred)
print(f'Accuracy: {accuracy:.2f}')
print('\nClassification Report:')
print(classification_report(y_test, y_test_pred))

# membagi data 70:30

X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['sentiment'], test_size=0.7, random_state=125)
vectorizer = TfidfVectorizer(max_features=200)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Terapkan SMOTE pada data latih untuk menangani ketidakseimbangan kelas
smote = SMOTE(random_state=42)
X_train_smote_tfidf, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)

# Inisialisasi dan pelatihan model Naive Bayes
nb_model = MultinomialNB()
nb_model.fit(X_train_smote_tfidf, y_train_smote)

# Prediksi pada data train
y_train_pred = nb_model.predict(X_train_smote_tfidf)
train_accuracy = accuracy_score(y_train_smote, y_train_pred)

# Prediksi pada data test
y_test_pred = nb_model.predict(X_test_tfidf)
test_accuracy = accuracy_score(y_test, y_test_pred)

# Menambahkan hasil ulasan klasifikasi ke dataframe
df['hasil_ulasan_klasifikasi_nb'] = nb_model.predict(vectorizer.transform(data['cleaned_text']))

# Matriks kebingungan dan laporan klasifikasi untuk data test
conf_matrix = confusion_matrix(y_test, y_test_pred)
class_report = classification_report(y_test, y_test_pred)

# Output hasil
print(f'Accuracy Train: {train_accuracy:.2f}\n')
print(f'Accuracy Test: {test_accuracy:.2f}\n')
print('Confusion Matrix:')
print(conf_matrix)
print('\nClassification Report:')
print(class_report)

# Evaluasi dengan Cross Validation
scores = cross_val_score(model, X_vectorized, y, cv=5)
print('Cross-validation scores:', scores)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pip install seaborn

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()  # Menampilkan plot

print(data['sentiment'].value_counts())

print(y_pred)

import matplotlib.pyplot as plt

# Assuming X_train and X_test represent 70% and 30% of the data respectively
train_size = 70  # representing 70% of data
test_size = 30   # representing 30% of data

# Preparing data for visualization
sizes = [train_size, test_size]
labels = ['Training Data (70%)', 'Testing Data (30%)']
colors = ['#66b3ff', '#99ff99']

# Creating pie chart
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, counterclock=False)
plt.title('Data Split Ratio: Training vs Testing')
plt.show()

import matplotlib.pyplot as plt

#ini adalah contoh jumlah sentimen yang telah dihitung
sentiment_counts = {
    'positif': 1800,
    'negatif': 150,
    'netral': 50  # Sentimen netral tidak akan ditampilkan
}

# 1. Pilih hanya sentimen positif dan negatif
filtered_sentiment = {key: sentiment_counts[key] for key in ['positif', 'negatif']}

# 2. Visualisasi - Diagram Batang Sentimen Positif dan Negatif
plt.figure(figsize=(8, 6))
plt.bar(filtered_sentiment.keys(), filtered_sentiment.values(), color=['gold', 'pink'])

# 3. Tambahkan judul dan label
plt.title('Distribusi Sentimen Positif dan Negatif')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')

# 4. Tampilkan plot
plt.show()

# Hitung jumlah setiap kategori sentimen
sentiment_counts = data['sentiment'].value_counts()

# Filter untuk hanya menampilkan sentimen positif dan negatif
filtered_sentiment_counts = sentiment_counts[sentiment_counts.index.isin(['positif', 'negatif'])]

# Visualisasi - Diagram Lingkaran (Pie Chart)
plt.figure(figsize=(8, 6))
plt.pie(filtered_sentiment_counts, labels=filtered_sentiment_counts.index, autopct='%1.1f%%',
        startangle=90, colors=['lightgreen', 'lightcoral'])
plt.title('Distribusi Sentimen (Positif dan Negatif)')
plt.axis('equal')  # Untuk membuat pie chart menjadi lingkaran sempurna
plt.show()

# Visualisasi Word Cloud
# Gabungkan token menjadi string untuk masing-masing sentimen
positive_text = ' '.join([' '.join(tokens) for tokens in data[data['sentiment'] == 'positif']['stemmed']])
negative_text = ' '.join([' '.join(tokens) for tokens in data[data['sentiment'] == 'negatif']['stemmed']])

# Membuat WordCloud untuk sentimen positif
wordcloud_positive = WordCloud(width=800, height=400, background_color='black', colormap='Greens').generate(positive_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud - Sentimen Positif')
plt.show()

# Membuat WordCloud untuk sentimen negatif
wordcloud_negative = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(negative_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud - Sentimen Negatif')
plt.show()

